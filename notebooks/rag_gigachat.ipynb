{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from transformers import AutoTokenizer\n",
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-17 16:02:04 config.py:1861] Downcasting torch.float32 to torch.float16.\n",
      "INFO 12-17 16:02:07 config.py:1020] Defaulting to use mp for distributed inference\n",
      "INFO 12-17 16:02:07 llm_engine.py:249] Initializing an LLM engine (v0.6.4.post1) with config: model='ai-sage/GigaChat-20B-A3B-instruct', speculative_config=None, tokenizer='ai-sage/GigaChat-20B-A3B-instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=24000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=ai-sage/GigaChat-20B-A3B-instruct, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, chat_template_text_format=string, mm_processor_kwargs=None, pooler_config=None)\n",
      "WARNING 12-17 16:02:08 multiproc_gpu_executor.py:56] Reducing Torch parallelism from 16 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 12-17 16:02:08 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "INFO 12-17 16:02:08 selector.py:135] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1920502)\u001b[0;0m INFO 12-17 16:02:08 selector.py:135] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1920502)\u001b[0;0m INFO 12-17 16:02:08 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 12-17 16:02:09 utils.py:961] Found nccl from library libnccl.so.2\n",
      "INFO 12-17 16:02:09 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1920502)\u001b[0;0m INFO 12-17 16:02:09 utils.py:961] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1920502)\u001b[0;0m INFO 12-17 16:02:09 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 12-17 16:02:09 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/stepan2/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1920502)\u001b[0;0m INFO 12-17 16:02:09 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/stepan2/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "INFO 12-17 16:02:09 shm_broadcast.py:236] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x795991118520>, local_subscribe_port=39361, remote_subscribe_port=None)\n",
      "INFO 12-17 16:02:09 model_runner.py:1072] Starting to load model ai-sage/GigaChat-20B-A3B-instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1920502)\u001b[0;0m INFO 12-17 16:02:09 model_runner.py:1072] Starting to load model ai-sage/GigaChat-20B-A3B-instruct...\n",
      "INFO 12-17 16:02:10 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1920502)\u001b[0;0m INFO 12-17 16:02:10 weight_utils.py:243] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4087e3c256a54b9eaebf279ea80fdeb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/17 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-17 16:09:09 model_runner.py:1077] Loading model weights took 19.2353 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1920502)\u001b[0;0m INFO 12-17 16:09:09 model_runner.py:1077] Loading model weights took 19.2353 GB\n",
      "WARNING 12-17 16:09:10 fused_moe.py:324] Using default MoE config. Performance might be sub-optimal! Config file not found at /home/stepan2/miniconda3/envs/llama-env/lib/python3.10/site-packages/vllm/model_executor/layers/fused_moe/configs/E=64,N=896,device_name=NVIDIA_RTX_A6000.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1920502)\u001b[0;0m WARNING 12-17 16:09:10 fused_moe.py:324] Using default MoE config. Performance might be sub-optimal! Config file not found at /home/stepan2/miniconda3/envs/llama-env/lib/python3.10/site-packages/vllm/model_executor/layers/fused_moe/configs/E=64,N=896,device_name=NVIDIA_RTX_A6000.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1920502)\u001b[0;0m INFO 12-17 16:09:15 worker.py:232] Memory profiling results: total_gpu_memory=47.54GiB initial_memory_usage=19.86GiB peak_torch_memory=20.89GiB memory_usage_post_profile=19.91GiB non_torch_memory=0.65GiB kv_cache_size=21.23GiB gpu_memory_utilization=0.90\n",
      "INFO 12-17 16:09:15 worker.py:232] Memory profiling results: total_gpu_memory=47.54GiB initial_memory_usage=19.87GiB peak_torch_memory=20.89GiB memory_usage_post_profile=19.92GiB non_torch_memory=0.67GiB kv_cache_size=21.22GiB gpu_memory_utilization=0.90\n",
      "INFO 12-17 16:09:15 distributed_gpu_executor.py:57] # GPU blocks: 24834, # CPU blocks: 4681\n",
      "INFO 12-17 16:09:15 distributed_gpu_executor.py:61] Maximum concurrency for 24000 tokens per request: 16.56x\n",
      "INFO 12-17 16:09:18 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 12-17 16:09:18 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1920502)\u001b[0;0m INFO 12-17 16:09:18 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1920502)\u001b[0;0m INFO 12-17 16:09:18 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1920502)\u001b[0;0m INFO 12-17 16:09:32 custom_all_reduce.py:224] Registering 1995 cuda graph addresses\n",
      "INFO 12-17 16:09:32 custom_all_reduce.py:224] Registering 1995 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1920502)\u001b[0;0m INFO 12-17 16:09:32 model_runner.py:1518] Graph capturing finished in 15 secs, took 1.50 GiB\n",
      "INFO 12-17 16:09:32 model_runner.py:1518] Graph capturing finished in 15 secs, took 1.50 GiB\n"
     ]
    }
   ],
   "source": [
    "model_name = \"ai-sage/GigaChat-20B-A3B-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "llm = LLM(model=model_name, trust_remote_code=True, tensor_parallel_size=2, max_model_len=24000)\n",
    "sampling_params = SamplingParams(temperature=0.3, max_tokens=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'house_dataset.csv'\n",
    "loader = CSVLoader(file_path=file_path, encoding=\"utf-8\")\n",
    "docs = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceTransformerEmbeddings:\n",
    "    def __init__(self, model_name=\"intfloat/multilingual-e5-large-instruct\"):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def embed_query(self, query):\n",
    "        return self.model.encode([query], convert_to_tensor=False)[0]\n",
    "\n",
    "    def embed_documents(self, docs):\n",
    "        return self.model.encode(docs, convert_to_tensor=False)\n",
    "\n",
    "embeddings = SentenceTransformerEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['939a7784-93af-4622-9cf3-8a260b4f64bb',\n",
       " 'c6d04531-d531-4267-ba22-04375a74c76d',\n",
       " 'eb9d9f8b-6e4e-4d41-95e6-7dfb9a614572',\n",
       " '8265b426-0419-4262-93d3-d5dc3d55ccc7',\n",
       " 'cb9fd0f7-866c-4d91-ba3b-5fd8826a1fcd',\n",
       " '993c87b5-3e84-4d16-9f09-c764b0ee2b00',\n",
       " 'a8607da7-4ef2-47c9-a794-5122b5f61c2d',\n",
       " 'fa5d86ad-b99d-4237-814a-8e6a617c92ac',\n",
       " '5f7850e2-32ea-48b7-8741-c4620b33dfaa',\n",
       " 'afa77ec4-0d81-4833-916a-971870ce020c',\n",
       " '32a931f8-3993-4b72-9b90-63c83a9865c8',\n",
       " '62dcd007-ac5a-40e2-89b4-9cb401d4fd0b',\n",
       " 'fb6acb11-e23a-47af-aad0-21e32cfc7523',\n",
       " '2b58c02a-e98a-4aff-a549-f9e4ab1036ce',\n",
       " '1c7f841a-444b-4003-ac28-8a79ea9b8b82',\n",
       " 'b797236c-d985-411d-b197-cd90a9536a89',\n",
       " 'b570b102-e3b9-44c3-8aff-688b57235ba1',\n",
       " '44a1b29f-f751-4345-9c44-ba44c8ad22cf',\n",
       " 'a07d2377-e018-48e1-8c94-bc94bb6f292d',\n",
       " '9446e35b-fffb-4110-af28-bb00ac591552',\n",
       " '5d9f538c-a866-4a9b-8551-596bf01aae2b',\n",
       " 'bb7574a8-eae1-4163-b0a4-d89c0aa90744',\n",
       " 'b9c6865f-44d2-4b49-8a75-546d7d1ebdc3',\n",
       " 'f86fbdfd-8e9e-4f34-bcf3-bad9fe02da8f',\n",
       " '7ad281a3-4ef1-4cb9-a198-b1d5c06a3b7d',\n",
       " '05b9f87c-fb88-4d15-bce5-91734ca280f6',\n",
       " '4253c04a-acb1-4947-98c1-db2092cbce34',\n",
       " 'be180658-4731-479c-94c3-c964062520db',\n",
       " '8a608051-7fd5-4c25-926d-6a76fb246a32',\n",
       " 'dd991066-22b1-414b-8d25-c1ce9bfbc85a',\n",
       " 'd1e313bf-ac3e-40cf-9467-123b078a6784',\n",
       " '9680ee08-0f32-48fb-8523-071ea25b03c2',\n",
       " '7965c416-d442-4f97-b41c-0f0837b7c551',\n",
       " 'ab56bcca-f3d0-466c-a7e8-cc76f316407e',\n",
       " 'df834916-37a7-4e76-8cc0-ae5b8591fd9f',\n",
       " 'd703a08c-fd22-44f8-bbdc-c381406efa9c',\n",
       " 'b159f951-db97-474e-ac9d-39a7722d4645',\n",
       " 'f673ab3c-1cff-4d7c-85df-9ca49e7c3291',\n",
       " 'dccd64d0-0211-4063-9cac-45ed7b928791',\n",
       " '0f75523a-d346-4c47-b73c-cfd13054d4b4',\n",
       " '92b21e1d-a75d-4c2c-9e53-cd1116a454f1',\n",
       " 'fc88b441-2225-428e-aadd-e417c5a2a7ff',\n",
       " 'a2512c21-dba7-4a38-b6f7-36c7845c00f7',\n",
       " 'feab319d-8d7b-435c-8a38-367862f17db5',\n",
       " 'f4341c1b-2a7e-41be-94b2-c084542df84a',\n",
       " 'a2c4649c-ade0-43ca-b8ac-dd19880860fe',\n",
       " '5e0bcbad-eeff-4985-b85c-d9fc1891ec49',\n",
       " 'aac89378-49f1-4645-a48e-79175b24028a',\n",
       " '371efc7b-ee74-4036-a68f-b98d2c8c9858',\n",
       " '9c5a82e6-3c7c-49bc-892a-86972a3ef3db',\n",
       " '5cafc165-14da-45e5-a6e5-7fe7dfb496ac',\n",
       " '9f740bce-dd78-447a-89a3-9b20202ac681',\n",
       " '0abe8803-7c15-4c11-8ce0-fc5dfabdfdc2',\n",
       " '9ffb2587-d120-413f-89fb-479d419a3b8e',\n",
       " 'ba7c622a-1c0d-4e73-8e39-ecd52a80da5e',\n",
       " '1cdeda0a-9590-45b8-8ded-dc85613a66e5',\n",
       " '173ea655-0652-4c69-a298-935e00bbda54',\n",
       " '88cb5a99-ae6e-4933-a69c-4bc611f68fe5',\n",
       " 'eba95426-af6a-4f23-948f-8772ccf77b2c',\n",
       " 'fce05a83-7e28-4d1c-8b93-96d12defd914',\n",
       " '29d8fa1e-9b45-48e5-aa00-f3b0870a0469',\n",
       " 'e572690d-3801-4570-9961-4338a917d880',\n",
       " 'f8790619-432b-446e-ab03-d7b9b0778199',\n",
       " '8a629fea-60ff-4029-a511-f30022e426a2',\n",
       " '42168ec6-f1ef-42f4-b314-6da7c20b6a57',\n",
       " '82ee6e80-44a4-421e-90a8-728ee62a324c',\n",
       " 'bf36e161-6109-4ed3-b89f-329bccbe9943',\n",
       " '15edc8cb-05aa-462f-b9a5-1f92c5675b4e',\n",
       " 'e08c4db3-ebcc-446f-94e3-399727c112da',\n",
       " '0c336c40-9aa8-4d4e-ae1b-78ea0ed2e8ce',\n",
       " '1f6d18a7-2e11-4fd6-808f-318e3df0912a',\n",
       " '84d5d16f-95da-4a5b-80b6-46bfc6a1c67d',\n",
       " 'a007d38d-2b36-4580-80dc-dd335786f412',\n",
       " 'a311ea92-fe4b-4bef-a8b2-a0bce71481ba',\n",
       " '8457cdcb-c532-4e22-9fb5-a21f24db9976',\n",
       " '08e012f7-bf8f-4672-86cb-403270a5fed6',\n",
       " '852d9c0b-e554-456d-953f-879f49a6077c',\n",
       " '1dda632c-8a8d-4310-8adc-d1be8b45d054',\n",
       " '5b6f06d5-98e7-46dd-93f5-54ff9d092967',\n",
       " '179b1511-c14c-46e0-9dc5-3c3f96fa223e',\n",
       " '117891fd-a712-4638-9f7f-7a8aa5293518',\n",
       " '3836b79c-c40f-4ea7-8bc6-8a5ae9092970',\n",
       " '1f5ceb7c-ed6a-417b-96c6-9969a7ec65cb',\n",
       " '21b4d860-6c31-4a7c-805a-fcff5eca1941',\n",
       " 'b1760c13-b25c-42fd-b634-bcfe5c9e642b',\n",
       " '9f85f178-78d9-4804-984f-05bfcc9f9242',\n",
       " '55fe7f60-d639-4088-81ff-652feb25e4f1',\n",
       " '65286211-6cbd-47a3-9139-d5503927af81',\n",
       " '124949a5-6789-4205-ad94-dc80c8d705d3',\n",
       " 'c63184ba-299e-4105-84a2-c61be6d5089f',\n",
       " 'f5974963-258a-4637-be60-ea1938a362eb',\n",
       " '46d743a1-dc3f-4369-83e9-d22c2278ab68',\n",
       " '160a2435-e66b-4b02-92e1-e174aa827547',\n",
       " 'e709df21-c966-48e1-8259-e4e1189f4fd8',\n",
       " 'f461e087-68e7-4816-b382-fe50b36a5f3b',\n",
       " '53e6d37b-0347-4a5e-b86e-56b0ccd97d95',\n",
       " 'be51b925-6c01-4204-b994-a16d44674baf',\n",
       " 'b5b4bafb-17ac-4911-98e8-c444b20f8e2d',\n",
       " '83dd2fdf-fea7-4273-a439-fa7a1451e532',\n",
       " '87625921-f069-4352-8429-3654798f17fe',\n",
       " '200697aa-82a9-43de-8022-166efe65e76f',\n",
       " '76210000-5a6c-40e1-a8fc-6faeb93beb93',\n",
       " '7227e6a3-d965-428c-abff-ae9424389c63',\n",
       " '65aaa6ea-5b58-493a-9863-973d14adfb13',\n",
       " 'ebfb60cf-5655-4d99-bd0a-6b3ad6fb3448',\n",
       " 'cc80c100-3805-44c7-beee-1c5a9ebd4484',\n",
       " '79a64f35-8400-4274-aa14-1fb48fab7487',\n",
       " '1b1ef057-9d34-46ac-b4ca-5f91679d5090',\n",
       " 'f02868a6-a14f-41b5-89b3-78ee6db9626e',\n",
       " 'bcba85b4-a963-4904-b26d-e977f2f0eeaa',\n",
       " '5368ec68-5c74-4a14-b589-d88e9e7ce342',\n",
       " '6c200945-8cb8-4a99-a18a-c4279629245e',\n",
       " 'd5ec1c25-eb59-4b0b-b6f9-eb7526d9a322',\n",
       " '11e784b8-514a-447d-a405-452664687f4c',\n",
       " 'ecc3ba5e-880d-4fda-a6fc-7c3dd9911e7e',\n",
       " 'ac159356-dc4c-4d8e-85e4-7ed47d24bdfa',\n",
       " 'b295c52d-7223-4b41-8b53-ee0259bca410',\n",
       " 'b75147df-bfbf-4649-b173-6f29d1e93e95',\n",
       " 'd6c0bedf-573a-439c-8236-053a39c703bf',\n",
       " 'eff15924-1509-4d2b-9ff8-cee2362bd735',\n",
       " '10049f3e-8845-41ae-8770-34100704b8db',\n",
       " '1210ba1d-d0d1-4630-8585-e330b5e9ff0b',\n",
       " 'f170073f-f869-4706-8ca8-9853e35de100',\n",
       " '394fe2ac-089e-470d-9bee-d5f059696968',\n",
       " '3fb477f8-6b5e-4e66-807b-6289b0a02507',\n",
       " '96e609f5-9a11-479e-88c1-cee31e538516',\n",
       " '2b74ff28-6558-4fc9-99e8-9c8a72188c92',\n",
       " '7933a99f-f73c-451a-8aaa-0550bcc70337',\n",
       " 'd238598b-d05b-4cff-a37b-b33a76ccf4d3',\n",
       " '33de9d72-be47-4bf6-8748-ff46c5181e29',\n",
       " 'e60fd7e1-204b-4a63-ba3e-a3606cffb3af',\n",
       " '3419ee5e-b2a7-4b15-b9bc-530f9ceb5581',\n",
       " 'b074e0bb-5c25-4a7b-be6f-dca875058a8c',\n",
       " 'dfa7df32-2c3b-460b-b2dc-ccd032f1f572',\n",
       " '2bd143ae-7389-4d93-a0a3-127c99cd77fe',\n",
       " '5854ab3f-a8dc-4ea8-9ad6-c4a69ddf4183',\n",
       " 'fbd53692-c34c-4667-b6ec-cf599037e72d',\n",
       " 'be176c44-428b-4783-877e-93eceea3b238',\n",
       " '57bb8393-35cb-4d1e-9231-744c9253b37d',\n",
       " 'd4886b60-2d24-4b15-a3b4-d7fa1f04ee01',\n",
       " 'f49f4873-1c6d-45a1-9547-fa7f9f238a49',\n",
       " '5bfce406-f4e9-4eb2-8ffd-5208243c1bd8',\n",
       " 'b9516ce2-7bd1-4113-a35d-a33b59d5f11d',\n",
       " '2c2750db-f688-4b96-ba40-4d5564af74eb',\n",
       " '7d0ec9c4-6b52-49e7-9d3e-c00354635031',\n",
       " 'ca270932-171f-442f-bd9f-c362d48570c8',\n",
       " '0cdb7c20-1752-488d-807e-c3c8975eb04f',\n",
       " '63805e08-0c66-40e8-9d46-fd4f417ccf32',\n",
       " 'ca9586f6-f439-4a78-b7ab-fd7c56562018',\n",
       " 'e94f5a56-a12e-4f81-a99a-2e6f2147a1de',\n",
       " '8a7641ae-034f-4d67-9940-bdcbea84d1cd',\n",
       " 'ebf01940-06f5-46d8-9065-9ccdf787d36b',\n",
       " '7cef5660-bf32-4841-b3f9-5e7d0ec9056d',\n",
       " '41eda7d2-2dab-4610-b162-2d90a72a6961',\n",
       " '0b3fcd05-7b52-4090-ab05-13923b5f5b7a',\n",
       " 'c145e3cd-8c61-4398-962f-1fc06478ad30',\n",
       " 'd3770ba8-5fd6-4854-b248-9eee6b6cf4d9',\n",
       " '6ec5f194-e9f6-43f2-a735-8ae7a679f165',\n",
       " '30aa93c9-30bb-4220-8f38-b64355a5d6f8',\n",
       " 'ab5edf96-983e-42bf-931d-138e5d89f7c8',\n",
       " '8519c2f6-a804-4aff-a141-1731c84ebc12',\n",
       " 'c257864d-a34a-4349-b975-a528267c7ed4',\n",
       " 'cce38d50-7bd4-44da-85ca-2f3efa57b5f3',\n",
       " '75b9974b-cac0-4a9b-9db2-1cb4f003412a',\n",
       " '2b863d57-2c9d-425d-9c56-da9c4add5b86',\n",
       " '6abec1f5-65b2-4b3a-baba-f147b5765165',\n",
       " '4d163324-ae2a-49d6-9f52-a02c24f99038',\n",
       " 'c861a3fa-b079-4029-8272-c5e8e81ded52',\n",
       " 'd17ef6a9-6e8f-4a3c-820d-e1c8d4756292',\n",
       " '73dd38e0-5c8b-40b8-a343-78b5ed9c2291',\n",
       " '1e9416d5-e294-40a9-91b4-0d84d3c30951',\n",
       " 'edd2b94b-d39f-4dfa-bf92-a1846188dc4e',\n",
       " 'd2d4940b-86c1-470c-a86c-67403c6c4f63',\n",
       " '066a7af5-f1ff-4f19-be25-f6d746da995e',\n",
       " 'fe33c5ff-20f0-4645-9648-17ec591994ca',\n",
       " '2270a867-c0a3-4d43-bada-70d43cc5c5cc',\n",
       " '695d6683-623f-40c2-8919-f6024881a233',\n",
       " '05922346-3547-417f-9ec5-7c6c63eae95c',\n",
       " '6fe92fc5-c2e9-4480-8bfa-c8442ddda835',\n",
       " 'f552e759-b931-4d29-b4de-3b637a9aa965',\n",
       " '336f22b6-eeeb-4a9e-a8d8-1ca863e5917a',\n",
       " '2673f33b-a6f0-4fca-959d-567e828ebc45',\n",
       " '421578ca-3f20-4adc-ae09-55368d588666',\n",
       " 'ddb07953-b64d-42e5-a5bf-c049ebf6230b',\n",
       " '2867ef5c-42d2-43e6-8a62-c9311b4aa7c0',\n",
       " '0b9cffe1-7520-4c87-90b5-a78ebbc14fdc',\n",
       " 'aaaf1384-4516-4273-9881-b7f434b0ec99',\n",
       " '1eabb76f-280e-4f87-9215-52291de23048',\n",
       " '50971513-e7be-4900-a3c2-9febe7c0c6d3',\n",
       " 'a60ebfba-2ab7-44b3-98bc-4e8c6d4c5abc',\n",
       " '38103348-05bf-488b-89e1-1068138b36ac',\n",
       " '7fb48433-aa09-4f8a-bc29-f864a7be7131',\n",
       " '9657ebd4-d3e6-40f2-b1d4-31c5127ff630',\n",
       " '8571ef30-a90b-4994-b2cf-9fd8564f2c76',\n",
       " '923f1f3b-e2cf-4152-b1c3-ab15e611bb40',\n",
       " 'f680650c-6668-43da-863a-927391e49094',\n",
       " '84e02596-fe4e-4e6d-9c6a-db1937d6c6b2',\n",
       " '9defe13a-9a47-4f6f-a0d8-12849af230d9',\n",
       " 'bd421cf1-0b48-48f8-b502-5cf5031e6a2f',\n",
       " 'ec78375d-5373-431c-b25a-d8ff02642e4c']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = len(embeddings.embed_query(\" \"))  \n",
    "index = faiss.IndexFlatL2(embedding_dim)\n",
    "vector_store = FAISS(\n",
    "    embedding_function=embeddings.embed_query,  \n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={}\n",
    ")\n",
    "\n",
    "texts = [doc.page_content for doc in docs]\n",
    "vector_store.add_texts(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vllm_chain(retriever, tokenizer, llm, sampling_params, system_message):\n",
    "    def answer_question(input_text):\n",
    "        \n",
    "        retrieved_docs = retriever.get_relevant_documents(input_text, k=10)\n",
    "        context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "        \n",
    "       \n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_message.content},\n",
    "            {\"role\": \"user\", \"content\": f\"Контекст: {context}\\n\\nВопрос: {input_text}\"}\n",
    "        ]\n",
    "        \n",
    "        \n",
    "        prompt_token_ids = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
    "        \n",
    "      \n",
    "        outputs = llm.generate(prompt_token_ids=[prompt_token_ids], sampling_params=sampling_params)\n",
    "        \n",
    "        \n",
    "        generated_text = outputs[0].outputs[0].text\n",
    "        return generated_text\n",
    "    \n",
    "    return answer_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = SystemMessage(content=\"Ты бот-помощник. Отвечай только на русском языке! Предоставь все релевантные данные о доме! ОБЯЗАТЕЛЬНО в конце отправляй номер телефона для связи и ссылку на объявление! Если нет идеально подходящего варианта, то обязательно скинь даже не самый лучший вариант!\")\n",
    "retriever = vector_store.as_retriever()\n",
    "vllm_chain = create_vllm_chain(retriever, tokenizer, llm, sampling_params, system_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1920169/2103615022.py:4: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retrieved_docs = retriever.get_relevant_documents(input_text, k=10)\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.86s/it, est. speed input: 828.05 toks/s, output: 77.86 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ответ: Предлагаю рассмотреть вариант аренды дома в Марушкинском поселении, д. Крекшино, улица Московская, 3А. Это место идеально подходит для проведения вечеринок и мероприятий. На первом этаже есть просторный зал с Алисой и Смарт ТВ, где можно установить пилон. Также есть кухня с необходимой техникой и мебелью. На втором этаже расположены три спальни и санузел с ванной. В доме могут разместиться до 12 человек. \n",
      "\n",
      "Для связи с владельцем дома, пожалуйста, используйте номер телефона +74952600803. \n",
      "\n",
      "Также, если вам интересен другой вариант, пожалуйста, уточните ваши предпочтения, и я постараюсь подобрать подходящий дом.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"нужен дом для вечеринки\"\n",
    "answer = vllm_chain(question)\n",
    "print(\"Ответ:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
